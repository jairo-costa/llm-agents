{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43d1966c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 16:56:32,251 [INFO] Environment setup completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# 0. Environment Setup — dependencies, dotenv loading, base imports\n",
    "\n",
    "# Install required packages (safe to run multiple times)\n",
    "!pip install -q markdown pypdf chromadb openai python-dotenv\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file (if present)\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging early so every later cell can use it\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"rag-prototype\")\n",
    "\n",
    "logger.info(\"Environment setup completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e1457f",
   "metadata": {},
   "source": [
    "# Project 03 — Retrieval-Augmented Generation (RAG) Prototype\n",
    "\n",
    "This notebook implements an end-to-end Retrieval-Augmented Generation (RAG)\n",
    "prototype to improve access to internal technical documentation.\n",
    "\n",
    "**Repository**\n",
    "- `llm-agents`\n",
    "\n",
    "**Environment**\n",
    "- Python (Jupyter notebook / GitHub Codespaces)\n",
    "- Vector store: ChromaDB\n",
    "- LLM provider: OpenAI API\n",
    "\n",
    "**Goal**\n",
    "Build a clear, well-documented prototype that:\n",
    "- loads real technical documents (PDF / Markdown),\n",
    "- preprocesses and chunks the content,\n",
    "- generates embeddings,\n",
    "- indexes them in a vector store (ChromaDB),\n",
    "- retrieves relevant chunks for a user query,\n",
    "- calls an LLM to generate grounded answers,\n",
    "- logs basic information for later quality review.\n",
    "\n",
    "> This is an internal prototype for exploration and discussion.\n",
    "> It is **not** a production-ready system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca607393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 16:56:32,259 [INFO] Core imports loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# 1. Core Imports — document parsing, vector store, LLM client, helpers\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import textwrap\n",
    "\n",
    "# Document parsing (PDF, Markdown)\n",
    "import markdown\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# Vector store (ChromaDB)\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# OpenAI client + error classes for safe handling\n",
    "from openai import OpenAI, RateLimitError, AuthenticationError\n",
    "\n",
    "logger.info(\"Core imports loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e29dd00d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "OPENAI_API_KEY is not set. Please define it in a .env file or export it as an environment variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m OPENAI_API_KEY = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OPENAI_API_KEY:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY is not set. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease define it in a .env file or export it as an environment variable.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m     )\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Initialize OpenAI client\u001b[39;00m\n\u001b[32m     13\u001b[39m openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
      "\u001b[31mValueError\u001b[39m: OPENAI_API_KEY is not set. Please define it in a .env file or export it as an environment variable."
     ]
    }
   ],
   "source": [
    "# 2. Configuration — environment variables, OpenAI client, directories, vector store\n",
    "\n",
    "# Load API key safely (coming from environment or .env)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\n",
    "        \"OPENAI_API_KEY is not set. \"\n",
    "        \"Please define it in a .env file or export it as an environment variable.\"\n",
    "    )\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Define the data directory for documents (PDF/Markdown)\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Initialize ChromaDB (local, in-memory by default)\n",
    "chroma_client = chromadb.Client(\n",
    "    Settings(\n",
    "        anonymized_telemetry=False,   # keep telemetry disabled\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create or load a collection for technical documents\n",
    "COLLECTION_NAME = \"technical_docs_rag\"\n",
    "collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "\n",
    "# Logging for transparency\n",
    "logger.info(\"Configuration loaded successfully.\")\n",
    "logger.info(\"DATA_DIR set to: %s\", DATA_DIR.resolve())\n",
    "logger.info(\"ChromaDB collection in use: %s\", COLLECTION_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98dfa48",
   "metadata": {},
   "source": [
    "## 3. Document loading\n",
    "\n",
    "In this prototype, we load internal technical documentation from a local\n",
    "`data/` directory.\n",
    "\n",
    "Supported formats:\n",
    "- `.pdf`\n",
    "- `.md` (Markdown)\n",
    "\n",
    "Each document is:\n",
    "1. Loaded from disk,\n",
    "2. Converted to plain text,\n",
    "3. Stored in a simple `Document` structure,\n",
    "4. Prepared for the chunking step in the next section.\n",
    "\n",
    "> In future iterations, this layer could be replaced by:\n",
    "> - cloud storage (e.g., S3, GCS),\n",
    "> - internal document management APIs,\n",
    "> - or automated exports from existing tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa12b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Helpers to load PDFs and Markdown files\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    doc_id: str\n",
    "    source_path: Path\n",
    "    text: str\n",
    "\n",
    "\n",
    "def load_pdf(path: Path) -> str:\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    logger.info(\"Loading PDF file: %s\", path)\n",
    "    reader = PdfReader(str(path))\n",
    "    pages_text = []\n",
    "\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text() or \"\"\n",
    "        pages_text.append(page_text)\n",
    "\n",
    "    return \"\\n\\n\".join(pages_text)\n",
    "\n",
    "\n",
    "def load_markdown(path: Path) -> str:\n",
    "    \"\"\"Convert Markdown file to a plain-text representation.\"\"\"\n",
    "    logger.info(\"Loading Markdown file: %s\", path)\n",
    "\n",
    "    raw = path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # Convert Markdown -> HTML\n",
    "    html = markdown.markdown(raw)\n",
    "\n",
    "    # Very lightweight HTML -> text\n",
    "    text = (\n",
    "        html.replace(\"<p>\", \"\\n\\n\")\n",
    "            .replace(\"</p>\", \"\")\n",
    "            .replace(\"<code>\", \"`\")\n",
    "            .replace(\"</code>\", \"`\")\n",
    "            .replace(\"<strong>\", \"\")\n",
    "            .replace(\"</strong>\", \"\")\n",
    "    )\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_documents(data_dir: Path) -> List[Document]:\n",
    "    \"\"\"Load all supported documents from the given directory.\"\"\"\n",
    "    docs = []\n",
    "\n",
    "    for path in sorted(data_dir.glob(\"**/*\")):\n",
    "        if path.is_dir():\n",
    "            continue\n",
    "\n",
    "        suffix = path.suffix.lower()\n",
    "\n",
    "        if suffix == \".pdf\":\n",
    "            text = load_pdf(path)\n",
    "        elif suffix == \".md\":\n",
    "            text = load_markdown(path)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        doc_id = path.stem\n",
    "        docs.append(Document(doc_id=doc_id, source_path=path, text=text))\n",
    "\n",
    "    logger.info(\"Loaded %d documents from %s\", len(docs), data_dir.resolve())\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6db0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 16:18:13,750 [INFO] Loaded 0 documents from /workspaces/llm-agents/Project 03 — Retrieval-Augmented Generation (RAG) Prototype/data\n",
      "2025-12-02 16:18:13,751 [WARNING] No documents found in /workspaces/llm-agents/Project 03 — Retrieval-Augmented Generation (RAG) Prototype/data. Please add PDFs or Markdown files into the data/ folder and re-run this cell.\n"
     ]
    }
   ],
   "source": [
    "# 3.2 Load and inspect documents\n",
    "\n",
    "documents = load_documents(DATA_DIR)\n",
    "\n",
    "if not documents:\n",
    "    logger.warning(\n",
    "        \"No documents found in %s. Please add PDFs or Markdown files into the data/ folder and re-run this cell.\",\n",
    "        DATA_DIR.resolve()\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Loaded %d documents.\", len(documents))\n",
    "    # Preview the first document (for debugging)\n",
    "    print(\"=== First document preview ===\\n\")\n",
    "    print(\"ID:\", documents[0].doc_id)\n",
    "    print(\"Source:\", documents[0].source_path)\n",
    "    print(\"\\n--- Text (first 500 chars) ---\\n\")\n",
    "    print(documents[0].text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1378ece0",
   "metadata": {},
   "source": [
    "## 4. Preprocessing and chunking\n",
    "\n",
    "RAG systems usually operate on **document chunks**, not on entire files.\n",
    "\n",
    "In this prototype we keep preprocessing intentionally simple and focus on:\n",
    "- preserving most of the original text, and\n",
    "- splitting content into manageable chunks for embeddings and retrieval.\n",
    "\n",
    "### Design choices (for this first version)\n",
    "\n",
    "- Light preprocessing (no aggressive cleaning).\n",
    "- Character-based chunking with a maximum size (e.g. ~1000 characters).\n",
    "- Prefer splitting on paragraph boundaries when possible.\n",
    "- Keep track of:\n",
    "  - `doc_id`\n",
    "  - `chunk_id`\n",
    "  - source path\n",
    "\n",
    "> In future iterations we can:\n",
    "> - switch to token-based splitting,\n",
    "> - add language-specific normalization,\n",
    "> - or plug into higher-level frameworks (LangChain, LlamaIndex, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb7c3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 16:20:26,971 [WARNING] No documents available to chunk. 'chunks' list is empty.\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Chunking utilities\n",
    "\n",
    "@dataclass\n",
    "class DocumentChunk:\n",
    "    doc_id: str\n",
    "    chunk_id: int\n",
    "    text: str\n",
    "    source_path: str\n",
    "\n",
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    doc_id: str,\n",
    "    source_path: Path,\n",
    "    max_chars: int = 1000,\n",
    ") -> List[DocumentChunk]:\n",
    "    \"\"\"\n",
    "    Split a long text into smaller chunks based on paragraphs,\n",
    "    keeping each chunk below `max_chars` characters.\n",
    "    \"\"\"\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "\n",
    "    chunks: List[DocumentChunk] = []\n",
    "    current_parts: List[str] = []\n",
    "    current_len = 0\n",
    "    chunk_id = 0\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_len = len(paragraph)\n",
    "\n",
    "        # If adding this paragraph keeps us under the limit, append\n",
    "        if current_len + paragraph_len + 2 <= max_chars:\n",
    "            current_parts.append(paragraph)\n",
    "            current_len += paragraph_len + 2\n",
    "        else:\n",
    "            # Close current chunk\n",
    "            if current_parts:\n",
    "                chunk_text_block = \"\\n\\n\".join(current_parts)\n",
    "                chunks.append(\n",
    "                    DocumentChunk(\n",
    "                        doc_id=doc_id,\n",
    "                        chunk_id=chunk_id,\n",
    "                        text=chunk_text_block,\n",
    "                        source_path=str(source_path),\n",
    "                    )\n",
    "                )\n",
    "                chunk_id += 1\n",
    "\n",
    "            # Start a new chunk with the current paragraph\n",
    "            current_parts = [paragraph]\n",
    "            current_len = paragraph_len\n",
    "\n",
    "    # Flush remaining text\n",
    "    if current_parts:\n",
    "        chunk_text_block = \"\\n\\n\".join(current_parts)\n",
    "        chunks.append(\n",
    "            DocumentChunk(\n",
    "                doc_id=doc_id,\n",
    "                chunk_id=chunk_id,\n",
    "                text=chunk_text_block,\n",
    "                source_path=str(source_path),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_documents(docs: List[Document], max_chars: int = 1000) -> List[DocumentChunk]:\n",
    "    \"\"\"\n",
    "    Apply `chunk_text` to all loaded documents.\n",
    "    \"\"\"\n",
    "    all_chunks: List[DocumentChunk] = []\n",
    "\n",
    "    for doc in docs:\n",
    "        doc_chunks = chunk_text(\n",
    "            text=doc.text,\n",
    "            doc_id=doc.doc_id,\n",
    "            source_path=doc.source_path,\n",
    "            max_chars=max_chars,\n",
    "        )\n",
    "        all_chunks.extend(doc_chunks)\n",
    "\n",
    "    logger.info(\n",
    "        \"Generated %d chunks from %d documents (max_chars=%d).\",\n",
    "        len(all_chunks),\n",
    "        len(docs),\n",
    "        max_chars,\n",
    "    )\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "# Run chunking if we have documents\n",
    "if documents:\n",
    "    chunks: List[DocumentChunk] = chunk_documents(documents, max_chars=1000)\n",
    "    logger.info(\"Example chunk: %s\", chunks[0] if chunks else \"No chunks created.\")\n",
    "else:\n",
    "    chunks = []\n",
    "    logger.warning(\"No documents available to chunk. 'chunks' list is empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b022e6",
   "metadata": {},
   "source": [
    "## 5. Embedding generation and vector store (ChromaDB)\n",
    "\n",
    "With the documents chunked, the next step is to convert each chunk into a\n",
    "numerical vector representation (embedding) and store it in a vector database.\n",
    "\n",
    "### Why embeddings?\n",
    "\n",
    "LLMs and RAG systems cannot search raw text efficiently.  \n",
    "Instead, each chunk is transformed into a vector that captures its semantic meaning.  \n",
    "These vectors allow similarity search (k-NN), enabling the system to find the\n",
    "chunks most relevant to a user query.\n",
    "\n",
    "### Why ChromaDB?\n",
    "\n",
    "For this prototype we use **ChromaDB** because it is:\n",
    "- lightweight,\n",
    "- fast,\n",
    "- easy to set up locally,\n",
    "- Python-first,\n",
    "- perfect for experimentation and internal demos.\n",
    "\n",
    "### Pipeline in this section:\n",
    "\n",
    "1. Generate embeddings using an OpenAI embedding model  \n",
    "2. Clear or initialize the Chroma collection (for reproducibility)  \n",
    "3. Upsert each chunk along with its metadata:\n",
    "   - `doc_id`\n",
    "   - `chunk_id`\n",
    "   - `source_path`\n",
    "4. Validate the number of items indexed  \n",
    "5. Prepare for retrieval in the next section\n",
    "\n",
    "> In future iterations we can add:\n",
    "> - hybrid search (sparse + dense),\n",
    "> - metadata filtering,\n",
    "> - alternative vector stores (Weaviate, Pinecone, Milvus),\n",
    "> - or custom embedding models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 16:25:27,891 [WARNING] No chunks found — skipping embedding + indexing.\n"
     ]
    }
   ],
   "source": [
    "# 5.1 Generate embeddings and index chunks into ChromaDB\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"   # lightweight, cost-efficient model\n",
    "\n",
    "\n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using the configured OpenAI model.\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating embeddings for %d texts...\", len(texts))\n",
    "\n",
    "    response = openai_client.embeddings.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=texts,\n",
    "    )\n",
    "\n",
    "    return [item.embedding for item in response.data]\n",
    "\n",
    "\n",
    "def index_chunks(chunks: List[DocumentChunk]) -> None:\n",
    "    \"\"\"\n",
    "    Index all chunks into the ChromaDB collection with:\n",
    "      - id\n",
    "      - embedding vector\n",
    "      - original text\n",
    "      - metadata (doc_id, chunk_id, source_path)\n",
    "    \"\"\"\n",
    "\n",
    "    if not chunks:\n",
    "        logger.warning(\"No chunks to index. Skipping embedding step.\")\n",
    "        return\n",
    "\n",
    "    # Optional: clear existing data to ensure reproducible runs\n",
    "    collection.delete(where={})\n",
    "    logger.info(\"Existing ChromaDB collection cleared.\")\n",
    "\n",
    "    batch_size = 64\n",
    "    total = len(chunks)\n",
    "\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = chunks[i : i + batch_size]\n",
    "        ids = [f\"{c.doc_id}-{c.chunk_id}\" for c in batch]\n",
    "        texts = [c.text for c in batch]\n",
    "        metadata = [\n",
    "            {\n",
    "                \"doc_id\": c.doc_id,\n",
    "                \"chunk_id\": c.chunk_id,\n",
    "                \"source_path\": c.source_path,\n",
    "            }\n",
    "            for c in batch\n",
    "        ]\n",
    "\n",
    "        embeddings = embed_texts(texts)\n",
    "\n",
    "        collection.add(\n",
    "            ids=ids,\n",
    "            embeddings=embeddings,\n",
    "            documents=texts,\n",
    "            metadatas=metadata,\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            \"Indexed batch %d–%d of %d chunks.\",\n",
    "            i,\n",
    "            min(i + batch_size - 1, total - 1),\n",
    "            total,\n",
    "        )\n",
    "\n",
    "    logger.info(\"Indexing completed. Total items in Chroma: %s\", collection.count())\n",
    "\n",
    "\n",
    "# Run indexing if chunks exist\n",
    "if chunks:\n",
    "    index_chunks(chunks)\n",
    "else:\n",
    "    logger.warning(\"No chunks found — skipping embedding + indexing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b9c405",
   "metadata": {},
   "source": [
    "## 6. RAG Pipeline — Retrieval + Generation\n",
    "\n",
    "With all chunks indexed in ChromaDB, we can now build the RAG pipeline.\n",
    "\n",
    "### High-level flow\n",
    "\n",
    "1. **User query**  \n",
    "   A natural language question is received.\n",
    "\n",
    "2. **Retrieval**  \n",
    "   ChromaDB performs a similarity search (k-NN) using the embedding of the query.\n",
    "   It returns the most relevant chunks based on semantic proximity.\n",
    "\n",
    "3. **Prompt construction**  \n",
    "   - Retrieved chunks are injected as context.  \n",
    "   - System instructions are applied to ensure grounded, concise answers.  \n",
    "   - Hallucination is discouraged by design.\n",
    "\n",
    "4. **LLM generation**  \n",
    "   The LLM receives the constructed prompt and generates a final answer\n",
    "   strictly based on the provided context.\n",
    "\n",
    "5. **Logging**  \n",
    "   For each query we store:\n",
    "   - the original question,\n",
    "   - the retrieved document IDs and metadata,\n",
    "   - the final model response.\n",
    "\n",
    "### Why this structure?\n",
    "\n",
    "This separation of responsibilities makes the prototype:\n",
    "- easy to debug,\n",
    "- easy to extend,\n",
    "- production-friendly,\n",
    "- and transparent for reviewers (leadership, engineers, auditors).\n",
    "\n",
    "> In future iterations we can add:\n",
    "> - hybrid search,\n",
    "> - ranking & scoring layers,\n",
    "> - evaluation datasets,\n",
    "> - retrieval metrics,\n",
    "> - or UI layers (e.g., Streamlit, Gradio).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c57dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 RAG core functions — retrieval, prompt building, LLM call, logging\n",
    "\n",
    "CHAT_MODEL = \"gpt-4.1-mini\"  # adjust if your org uses a different default model\n",
    "\n",
    "# In-memory log of RAG interactions (for later inspection / evaluation)\n",
    "rag_logs: List[Dict[str, Any]] = []\n",
    "\n",
    "\n",
    "def retrieve_context(query: str, k: int = 4) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retrieve top-k most relevant chunks from ChromaDB for a given query.\n",
    "    \"\"\"\n",
    "    logger.info(\"Retrieving context for query: %s\", query)\n",
    "\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=k,\n",
    "    )\n",
    "\n",
    "    # ChromaDB returns lists-of-lists (one per query)\n",
    "    ids = results.get(\"ids\", [[]])[0]\n",
    "    documents = results.get(\"documents\", [[]])[0]\n",
    "    metadatas = results.get(\"metadatas\", [[]])[0]\n",
    "\n",
    "    logger.info(\"Retrieved %d chunks from ChromaDB.\", len(ids))\n",
    "\n",
    "    return {\n",
    "        \"ids\": ids,\n",
    "        \"documents\": documents,\n",
    "        \"metadatas\": metadatas,\n",
    "    }\n",
    "\n",
    "\n",
    "def build_prompt(query: str, retrieved: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Build a grounded prompt using the retrieved context + user question.\n",
    "    \"\"\"\n",
    "    context_blocks = []\n",
    "\n",
    "    for meta, doc in zip(retrieved[\"metadatas\"], retrieved[\"documents\"]):\n",
    "        header = f\"[doc_id={meta.get('doc_id')} chunk_id={meta.get('chunk_id')}]\"\n",
    "        block = f\"{header}\\n{doc}\"\n",
    "        context_blocks.append(block)\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join(context_blocks) if context_blocks else \"No relevant context found.\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a technical assistant answering questions based ONLY on the documentation provided below.\n",
    "\n",
    "If the documentation does not contain enough information to answer confidently,\n",
    "say that the information is not available in the current documents.\n",
    "\n",
    "# Documentation\n",
    "\n",
    "{context_text}\n",
    "\n",
    "# User question\n",
    "\n",
    "{query}\n",
    "\n",
    "# Instructions\n",
    "- Answer in a concise and clear way.\n",
    "- Reference relevant document ids and/or chunk ids when helpful.\n",
    "- Do not invent features or details that are not supported by the documentation.\n",
    "    \"\"\".strip()\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def answer_question(query: str, k: int = 4) -> str:\n",
    "    \"\"\"\n",
    "    Full RAG pipeline:\n",
    "    1) retrieve context from ChromaDB\n",
    "    2) build a grounded prompt\n",
    "    3) call the LLM\n",
    "    4) log query, retrieved metadata and answer\n",
    "\n",
    "    This function also handles common API issues gracefully:\n",
    "    - invalid / missing API key (AuthenticationError)\n",
    "    - quota / rate-limit problems (RateLimitError)\n",
    "    \"\"\"\n",
    "    retrieved = retrieve_context(query, k=k)\n",
    "    prompt = build_prompt(query, retrieved)\n",
    "\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=CHAT_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful technical assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "        )\n",
    "    except AuthenticationError as e:\n",
    "        logger.error(\"Authentication error when calling the LLM API: %s\", e)\n",
    "        return (\n",
    "            \"Authentication error when calling the LLM API. \"\n",
    "            \"Please check if the API key is valid and correctly configured.\"\n",
    "        )\n",
    "    except RateLimitError as e:\n",
    "        logger.error(\"Rate limit / quota error when calling the LLM API: %s\", e)\n",
    "        return (\n",
    "            \"Rate limit / quota error when calling the LLM API. \"\n",
    "            \"Please verify the API quota / billing configuration for this key.\"\n",
    "        )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    # Store a compact log entry for later review\n",
    "    log_entry = {\n",
    "        \"query\": query,\n",
    "        \"retrieved_ids\": retrieved[\"ids\"],\n",
    "        \"retrieved_metadatas\": retrieved[\"metadatas\"],\n",
    "        \"answer\": answer,\n",
    "    }\n",
    "    rag_logs.append(log_entry)\n",
    "\n",
    "    logger.info(\"Query answered successfully. Retrieved chunks: %d\", len(retrieved[\"ids\"]))\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eacc17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 16:30:22,619 [INFO] Retrieving context for query: What is the main purpose of the system described in the documentation?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUESTION:\n",
      "What is the main purpose of the system described in the documentation?\n",
      "\n",
      "ANSWER:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 16:30:22,960 [INFO] HTTP Request: GET https://chroma-onnx-models.s3.amazonaws.com/all-MiniLM-L6-v2/onnx.tar.gz \"HTTP/1.1 200 OK\"\n",
      "/home/codespace/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:00<00:00, 104MiB/s] \n",
      "2025-12-02 16:30:24,879 [INFO] Retrieved 0 chunks from ChromaDB.\n",
      "2025-12-02 16:30:25,339 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 401 Unauthorized\"\n",
      "2025-12-02 16:30:25,340 [ERROR] Authentication error when calling the LLM API: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-xxxxx***********************xxxx. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication error when calling the LLM API. Please check if the API key is valid and correctly configured.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 6.2 Manual smoke test for the RAG pipeline\n",
    "\n",
    "test_questions = [\n",
    "    \"What is the main purpose of the system described in the documentation?\",\n",
    "    # You can add more questions here if desired, e.g.:\n",
    "    # \"How is authentication handled according to the docs?\",\n",
    "    # \"Are there any limitations or known issues mentioned?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"QUESTION:\")\n",
    "    print(q)\n",
    "    print(\"\\nANSWER:\\n\")\n",
    "\n",
    "    answer = answer_question(q)\n",
    "    print(answer)\n",
    "    print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250024d3",
   "metadata": {},
   "source": [
    "## 7. Next steps and known limitations\n",
    "\n",
    "This notebook implements a full end-to-end RAG prototype with:\n",
    "\n",
    "- document loading (PDF / Markdown),\n",
    "- chunking,\n",
    "- embedding generation,\n",
    "- vector store indexing (ChromaDB),\n",
    "- retrieval,\n",
    "- grounded prompt construction,\n",
    "- LLM-based answer generation,\n",
    "- and structured logging for review.\n",
    "\n",
    "### Known limitations (expected for a prototype)\n",
    "\n",
    "- The pipeline uses a simple character-based chunker.\n",
    "- No metadata filtering or ranking layer is applied.\n",
    "- Evaluation metrics (recall, precision, mAP) are not included.\n",
    "- The LLM call is synchronous and single-turn.\n",
    "- ChromaDB runs locally and is not persistent across sessions.\n",
    "- No UI layer (CLI / Streamlit / Gradio) has been added yet.\n",
    "- Error handling covers only the most common API issues.\n",
    "\n",
    "### Possible upcoming improvements\n",
    "\n",
    "- Add token-based chunking (tiktoken or other tokenizer).\n",
    "- Integrate a scalable vector store (Weaviate, Pinecone, Milvus).\n",
    "- Implement hybrid search (BM25 + embeddings).\n",
    "- Add RAG evaluation datasets (question/answer pairs).\n",
    "- Introduce reranking (cross-encoders, ColBERT, LLM-based re-ranking).\n",
    "- Build a simple UI to demonstrate the end-user experience.\n",
    "- Containerize the prototype (Dockerfile + requirements + entrypoint).\n",
    "- Add monitoring (latency, similarity heatmaps, retrieval diagnostics).\n",
    "\n",
    "### Final note\n",
    "\n",
    "This notebook is intentionally clean, minimal and education-oriented.\n",
    "It is designed to serve as a baseline for further refinement and as a\n",
    "transparent demonstration of how a simple RAG pipeline works end-to-end.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
